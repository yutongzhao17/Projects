{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1r5gV29LLWJ"
   },
   "source": [
    "# Part II: Sequence to Sequence Models in tensorflow\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwjBYH6hgPRP"
   },
   "source": [
    "## Task: Language Modeling\n",
    "Today we will experiment with using tensorflow to build a very simple seq2seq structure for language modeling.\n",
    "### Definition\n",
    "We input a sequence of words/characters to an RNN so that it can learn the probability distribution of the next word/character in the sequence given the history of previous characters. This will then allow us to generate text one unit at a time.\n",
    "\n",
    "We will use 全唐诗 as our training data, and try to generate new poems later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnXFHqP7fmJw"
   },
   "source": [
    "## Check the content\n",
    "\n",
    "I have already uploaded the poem text to this server. We need to first do some preprocessing.\n",
    "\n",
    "** Confirm the correctness of preprocessing **\n",
    "\n",
    "When you deal with your own dataset, you have to write your own preprocessing procedure. There are all kinds of noise in text data. Remember to check the correctness of the preprocessing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0bI7rsH2LU_L"
   },
   "source": [
    "The format of our input data is like this:\n",
    "\n",
    "`(optional title + \":\")poem`\n",
    "\n",
    "We will use only the poem part and not the title.\n",
    "\n",
    "However, some special cases like:\n",
    "\n",
    "* 河鱼未上冻，江蛰已闻雷。（见《纬略》）\n",
    "* □□□□□\n",
    "\n",
    "We need some preprocessing as mentioned in our last codelab.\n",
    "\n",
    "* Remove title\n",
    "* Remove spaces\n",
    "* Remove empty symbols\n",
    "* Replace other symbols\n",
    "\n",
    "Finally, we will randomize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDxfxaNbE1Na"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "data_filename ='poetry.txt'\n",
    "poems = []\n",
    "with open(data_filename, \"r\") as in_file:\n",
    "  for line in in_file.readlines():\n",
    "    line = line.strip()\n",
    "    # find title if exists\n",
    "    if ':' in line:\n",
    "      line = line.split(':')\n",
    "    # some poems are empty\n",
    "    if len(line) == 2:\n",
    "      poem = line[1]\n",
    "    else:\n",
    "      continue\n",
    "    # discard if contains special symbols\n",
    "    if re.search(r'[(（《_□]', poem):\n",
    "      continue\n",
    "    # discard if too short or too long\n",
    "    if len(poem) < 5 or len(poem) > 40:\n",
    "      continue\n",
    "    # remove symbols\n",
    "    poem = re.sub(u'[，。]','',poem)\n",
    "    poems.append(poem)\n",
    "\n",
    "poems = np.random.permutation(poems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFjPmxWXlZJ2"
   },
   "source": [
    "We select 5 poems as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqbNdpZYO8ma"
   },
   "outputs": [],
   "source": [
    "poems_train, poems_test = poems[:-5], poems[-5:]\n",
    "len(poems_train), len(poems_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb9em1KyfuvZ"
   },
   "source": [
    "## Word to ID\n",
    "\n",
    "As mentioned lst time, we can use the tokenizer in Keras to help us. We set\n",
    "```python\n",
    "Tokenizer(num_words=None, lower=False, char_level=True)\n",
    "```\n",
    "to not limit the number of words in the dictionary, and use character as our unit.\n",
    "\n",
    "We also need to correct the dictionary because it starts from 1 and that will be a problem later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1zXYlrxPMPx"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "poem_tokenizer = Tokenizer(num_words=None, lower=False, char_level=True)\n",
    "# Create word to ID dictionary\n",
    "poem_tokenizer.fit_on_texts(poems)\n",
    "# Get dictionary\n",
    "word_index = poem_tokenizer.word_index\n",
    "\n",
    "# Note that ID starts from 1!!\n",
    "# We need to add special ID 0\n",
    "word_index[\"<PAD>\"] = 0\n",
    "# Create ID to word \n",
    "reverse_word_index = dict([(v, k) for (k, v) in word_index.items()])\n",
    "print(\"Number of unique chars: {}\".format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29iNZFMwKzB-"
   },
   "source": [
    "Again, always check if there is any strange symbols in the dictionary. Here we only print first and last parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Bqy83LtQWjA"
   },
   "outputs": [],
   "source": [
    "# sort word index by ID\n",
    "for (w,i) in sorted(word_index.items(), key=lambda w: w[1]):\n",
    "# print some words to check if there are errors!\n",
    "  if i > 10 and i < len(word_index)-5: continue\n",
    "  print(\"{} {}\".format(w,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPVYU_l3qUq_"
   },
   "outputs": [],
   "source": [
    "# Apply word to ID on training and test set\n",
    "poems_train = poem_tokenizer.texts_to_sequences(poems_train)\n",
    "poems_test = poem_tokenizer.texts_to_sequences(poems_test)\n",
    "# Check and see if there is any error\n",
    "print(poems_train[0])\n",
    "print(''.join([reverse_word_index[w] for w in poems_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPBqvnAKf0Me"
   },
   "source": [
    "## Prepare the data for input\n",
    "\n",
    "We flatten the input to a long list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reJzVQlkLgs_"
   },
   "outputs": [],
   "source": [
    "# flatten to a long string of characters\n",
    "poems_train = [w for po in poems_train for w in po]\n",
    "\n",
    "# flatten to a long string of characters\n",
    "poems_test = [w for po in poems_test for w in po]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rj9yNonAf49F"
   },
   "source": [
    "## Define an input object\n",
    "\n",
    "We need to put the input into batches.\n",
    "* Reshape input data into a rectangular matrix and crop remainders\n",
    "* Calculate shape of each batch\n",
    "* Generate batch with input and output = input shift by one time step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXoDJKqbTYnz"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNVdwFpgmoeq"
   },
   "outputs": [],
   "source": [
    "class PoemInput(object):\n",
    "  def __init__(self, data, config, name=None):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = num_steps = config.num_steps\n",
    "    self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "    self.sources, self.targets = self.input_producer(\n",
    "        data, batch_size, num_steps, name=name)\n",
    "\n",
    "  def input_producer(self, raw_data, batch_size, num_steps, name=None):\n",
    "    \"\"\"Reshape the poem data to form input and output.\n",
    "    This chunks the raw_data into batches of examples and returns Tensors that\n",
    "    are drawn from these batches.\n",
    "    Args:\n",
    "      raw_data: a list of words\n",
    "      batch_size: int, the batch size.\n",
    "      num_steps: int, the sequence length.\n",
    "      name: the name of this operation (optional).\n",
    "    Returns:\n",
    "      A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "      of the tuple is the same data time-shifted to the right by one.\n",
    "    \"\"\"\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "    # get size of the 1-d tensor\n",
    "    data_len = tf.size(raw_data)\n",
    "    # calculate how many batches\n",
    "    batch_len = data_len // batch_size\n",
    "    # crop data that does not fit in a batch\n",
    "    data = tf.reshape(raw_data[0:batch_size*batch_len],\n",
    "                      [batch_size, batch_len])\n",
    "    # calculate how many batches in an epoch\n",
    "    epoch_size = (batch_len-1) // num_steps\n",
    "    # make sure there is at least one batch\n",
    "    assertion = tf.assert_positive(epoch_size,\n",
    "        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    with tf.control_dependencies([assertion]):\n",
    "      epoch_size = tf.cast(tf.identity(epoch_size, name=\"epoch_size\"), tf.int64)\n",
    "\n",
    "    # start generating slices\n",
    "    # range_input_producer returns a sequence of IDs \n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = data[:, i*num_steps  :(i+1)*num_steps]\n",
    "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6k4i6WJ8gFLl"
   },
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhBnCcTTUpLS"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "class Hparam(object):\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 1\n",
    "  num_steps = 35\n",
    "  vocab_size = len(word_index)\n",
    "  embedding_size = 100\n",
    "  hidden_size = 100\n",
    "  warmup_epochs = 3\n",
    "  num_epochs_to_train = 5\n",
    "  keep_prob = 0.6\n",
    "  lr_decay = 0.9\n",
    "  batch_size = 100\n",
    "\n",
    "config = Hparam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyOWLTIVgJWZ"
   },
   "source": [
    "## Construct model\n",
    "In this step, the entire model structure must be defined completely. Including\n",
    "* Input\n",
    "* Size of layers\n",
    "* Connection between layers\n",
    "* Variables in layers\n",
    "* Output\n",
    "* Loss\n",
    "* Operations that apply the gradients (optimizer)\n",
    "* Placeholder for feeding special values\n",
    "* Properties that can be read from outside\n",
    "\n",
    "Note that we will use CudnnLSTM to speed up our training if available. However, I will provide two versions of LSTM here in case you cannot find a machine with GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5tuOJamJS6gm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell\n",
    "from tensorflow.nn import embedding_lookup, dropout\n",
    "\n",
    "# Build our model\n",
    "class MySeq2SeqModel(object):\n",
    "  def __init__(self, is_training, config, input_):\n",
    "    self._is_training = is_training\n",
    "    self._input = input_\n",
    "    self._cell = None\n",
    "    self.batch_size = input_.batch_size\n",
    "    self.num_steps = input_.num_steps\n",
    "    rnn_size = config.hidden_size\n",
    "    vocab_size = config.vocab_size\n",
    "    embedding_size = config.embedding_size\n",
    "\n",
    "    # Embeddings can only exist on CPU\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "      embedding_weights = tf.get_variable(\"embedding\", \\\n",
    "                     [vocab_size, embedding_size])\n",
    "      embed_inputs = tf.nn.embedding_lookup(embedding_weights, input_.sources)\n",
    "\n",
    "    if is_training and config.keep_prob < 1.:\n",
    "      embed_inputs = tf.nn.dropout(embed_inputs, config.keep_prob)\n",
    "\n",
    "    # build RNN using CudnnLSTM\n",
    "    output, _ = self._build_rnn(embed_inputs, config, is_training)\n",
    "    # build RNN using basic LSTM\n",
    "    # output, _ = self._build_rnn_old_lstm(embed_inputs, config, is_training)\n",
    "\n",
    "    # Remember RNN output is [batch_size x time, rnnsize]\n",
    "    # Dense layer for projecting onto vocabulary size\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "    # Reshape logits to be a 3-D tensor for sequence loss\n",
    "    logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "    self._logits = logits\n",
    "\n",
    "    # Use the contrib sequence loss and average over the batches\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        input_.targets,\n",
    "        tf.ones([self.batch_size, self.num_steps]),\n",
    "        average_across_timesteps=False,\n",
    "        average_across_batch=True)\n",
    "\n",
    "    # Update the cost\n",
    "    self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "    if not is_training:\n",
    "      return\n",
    "\n",
    "    # A variable to store learning rate\n",
    "    self._lr = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "    # Calculate gradients\n",
    "    # Get a list of trainable variables\n",
    "    tvars = tf.trainable_variables()\n",
    "    # Get gradient and clip by norm\n",
    "    grads, _ = tf.clip_by_global_norm(\\\n",
    "                 tf.gradients(self._cost, tvars),\n",
    "                 config.max_grad_norm)\n",
    "    # Define an optimizer\n",
    "    # Note that the optimizer reads the value of learning rate from variable\n",
    "    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "    # Define an operation that actually applies the gradients\n",
    "    self._train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=tf.train.get_or_create_global_step())\n",
    "    # A placeholder for feeding new learning rates\n",
    "    self._new_lr = tf.placeholder(\n",
    "         tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "    self._lr_update_op = tf.assign(self._lr, self._new_lr)\n",
    "  \n",
    "  def _build_rnn(self, inputs, config, is_training):\n",
    "    # RNN requires time-major\n",
    "    inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "    self._cell = CudnnLSTM(\n",
    "        num_layers=config.num_layers,\n",
    "        num_units=config.hidden_size,\n",
    "        )\n",
    "    self._cell.build(inputs.get_shape())\n",
    "    outputs, state = self._cell(inputs)\n",
    "    # Transpose from time-major to batch-major\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    # Reshape from [batch, time, rnnsize] to [batch x time, rnnsize]\n",
    "    # For computing softmax later\n",
    "    outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
    "    return outputs, state\n",
    "\n",
    "  def _build_rnn_old_lstm(self, inputs, config, is_training):\n",
    "    def make_cell():\n",
    "      cell = BasicLSTMCell(\n",
    "        config.hidden_size, forget_bias=0.0, state_is_tuple=True,\n",
    "        reuse=not is_training)\n",
    "      if is_training and config.keep_prob < 1:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            cell, output_keep_prob=config.keep_prob)\n",
    "      return cell\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [make_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "\n",
    "    self._initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "    state = self._initial_state\n",
    "    outputs = []\n",
    "    inputs = tf.unstack(inputs, num=self.num_steps, axis=1)\n",
    "    outputs, state = tf.nn.static_rnn(cell, inputs,\n",
    "                                      initial_state=self._initial_state)\n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n",
    "    return output, state\n",
    "  \n",
    "  def assign_lr(self, session, lr_value):\n",
    "    session.run(self._lr_update_op, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "  @property\n",
    "  def input(self):\n",
    "    return self._input\n",
    "\n",
    "  @property\n",
    "  def cost(self):\n",
    "    return self._cost\n",
    "\n",
    "  @property\n",
    "  def lr(self):\n",
    "    return self._lr\n",
    "\n",
    "  @property\n",
    "  def train_op(self):\n",
    "    return self._train_op\n",
    "\n",
    "  @property\n",
    "  def logits(self):\n",
    "    return self._logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EYipAzwgXMD"
   },
   "source": [
    "## Define a training operation for an epoch\n",
    "This procedure gets the output from the model for each batch.\n",
    "We need a dictionary with these keys:\n",
    "\n",
    "* \"cost\": Reads the propertie `model.cost` that we defined above. \n",
    "* \"do_op\": Perform operation `model.train_op` that applies gradients\n",
    "\n",
    "After running (calling `session.run()`), the same key will contain the return values.\n",
    "\n",
    "We can add any key in the dictionary that corresponds to `@property` in the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84z2L2sggp48"
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, do_op=None, verbose=False):\n",
    "  start_time = time.time()\n",
    "  costs = 0.0\n",
    "  iters = 0\n",
    "  feed_to_model_dict = {\n",
    "      \"cost\": model.cost,\n",
    "  }\n",
    "  # if an operation is provided, put that in the feed\n",
    "  if do_op is not None:\n",
    "    feed_to_model_dict[\"do_op\"] = do_op\n",
    "\n",
    "  for step in range(model.input.epoch_size):\n",
    "    # use the session to run, feed the dictionary\n",
    "    s_out = session.run(feed_to_model_dict)\n",
    "    # The returned dictionary will contain the information we need\n",
    "    cost = s_out[\"cost\"]\n",
    "    # Accumulate cost\n",
    "    costs += cost\n",
    "    # Accumulate number of training steps\n",
    "    iters += model.input.num_steps\n",
    "    # Print loss periodically\n",
    "    if verbose and (step+1) % (model.input.epoch_size // 5) == 0:\n",
    "      print(\"%.0f%% ppl: %.3f, speed: %.0f char/sec\" %\n",
    "            ((step+1) * 100.0 / model.input.epoch_size, \\\n",
    "             np.exp(costs/iters), \\\n",
    "             iters * model.input.batch_size/(time.time() - start_time)))\n",
    "\n",
    "  return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHRlVdNelCkx"
   },
   "source": [
    "## Define a Generator\n",
    "We will also create a Generator Model to generate new poems. Note that it is much less complicated than the training model.\n",
    "However, we need to add a procedure to generate output for some steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZ0VsXVuqBM1"
   },
   "outputs": [],
   "source": [
    "class MyGeneratorModel(object):\n",
    "  def __init__(self, config):\n",
    "    self._input = tf.placeholder(tf.int32, shape=[1], name=\"_input\")\n",
    "    self.batch_size = 1\n",
    "    self.num_steps = config.num_steps\n",
    "    rnn_size = config.hidden_size\n",
    "    vocab_size = config.vocab_size\n",
    "    embedding_size = config.embedding_size\n",
    "\n",
    "    # Embeddings can only exist on CPU\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "      embedding_weights = tf.get_variable(\"embedding\", \\\n",
    "                     [vocab_size, embedding_size])\n",
    "      embed_inputs = tf.nn.embedding_lookup(embedding_weights, self._input)\n",
    "      embed_inputs = tf.expand_dims(embed_inputs, 0)\n",
    "\n",
    "    # build RNN using CudnnLSTM\n",
    "    self._cell = CudnnLSTM(\n",
    "        num_layers=config.num_layers,\n",
    "        num_units=config.hidden_size,\n",
    "        )\n",
    "\n",
    "    # build final projection layer\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "\n",
    "    # Collect a sequence of output word IDs\n",
    "    self._output_word_ids = []\n",
    "\n",
    "    # Decode first word\n",
    "    outputs, state = self._cell(embed_inputs)\n",
    "    outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
    "    logits = tf.nn.xw_plus_b(outputs, softmax_w, softmax_b)\n",
    "    # Get input for next step\n",
    "    next_input = tf.argmax(logits, axis=-1)\n",
    "    next_input = tf.squeeze(next_input)\n",
    "    self._output_word_ids.append(next_input)\n",
    "    # Convert next input to word embeddings\n",
    "    next_input = tf.nn.embedding_lookup(embedding_weights, next_input)\n",
    "    next_input = tf.reshape(next_input, [1, 1, embedding_size])\n",
    "    \n",
    "    # Feed back to LSTM\n",
    "    for _ in range(self.num_steps-1):\n",
    "      outputs, state = self._cell(next_input, state)\n",
    "      outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
    "      logits = tf.nn.xw_plus_b(outputs, softmax_w, softmax_b)\n",
    "      next_input = tf.argmax(logits, axis=-1)\n",
    "      next_input = tf.squeeze(next_input)\n",
    "      self._output_word_ids.append(next_input)\n",
    "\n",
    "      next_input = tf.nn.embedding_lookup(embedding_weights, next_input)\n",
    "      next_input = tf.reshape(next_input, [1, 1, embedding_size])\n",
    "\n",
    "  @property\n",
    "  def output_word_ids(self):\n",
    "    return self._output_word_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oOtiwCr0lrD"
   },
   "source": [
    "## Define a call to generator\n",
    "Again we need a decoder to translate word IDs back to words. And we need to define a procedure to communicate with the model. `feed_dict` and `fetches` are two keys to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CB9TLfWilDg9"
   },
   "outputs": [],
   "source": [
    "def decode_text(text, max_len_newline=5):\n",
    "  words = [reverse_word_index.get(i, \"<UNK>\") for i in text]\n",
    "  fixed_width_string = []\n",
    "\n",
    "  for w_pos in range(len(words)):\n",
    "    fixed_width_string.append(words[w_pos])\n",
    "    if (w_pos+1) % max_len_newline == 0:\n",
    "      fixed_width_string.append('\\n')\n",
    "  return ''.join(fixed_width_string)\n",
    "\n",
    "def run_generator(session, model, seed_word, config):\n",
    "  \n",
    "  feed_to_model_dict = {\n",
    "      model._input: [seed_word],\n",
    "  }\n",
    "  fetch_model_dict = {\n",
    "      \"output_word_ids\": model.output_word_ids\n",
    "  }\n",
    "\n",
    "  # An example of sending and receiving data from the model\n",
    "  vals = session.run(fetches=fetch_model_dict, feed_dict=feed_to_model_dict)\n",
    "  output_word_ids = vals['output_word_ids']\n",
    "\n",
    "  # Decode to readable words\n",
    "  print(decode_text([seed_word] + output_word_ids, (config.num_steps+1)//4))\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qvm0wW2inUwk"
   },
   "source": [
    "## Main training controller\n",
    "Finally, we define a controller that:\n",
    "* Create the model for training\n",
    "* Create the model for testing, copying from the training model\n",
    "* Prepare the input data\n",
    "* Define what to log in the progress of training\n",
    "* Create a `session` that communicates with computation graph\n",
    "* Change learning rate optionally\n",
    "* Get test set results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKsN1qiVhdms"
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "      # Create input producer\n",
    "      train_input = PoemInput(poems_train, config, name=\"TrainInput\")\n",
    "      # Create the model instance\n",
    "      with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "        m = MySeq2SeqModel(is_training=True, config=config, input_=train_input)\n",
    "      # Add information to logs\n",
    "      tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "      tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "\n",
    "    with tf.name_scope(\"Test\"):\n",
    "      eval_config = Hparam()\n",
    "      eval_config.batch_size = 1\n",
    "      eval_config.num_steps = 20\n",
    "\n",
    "      # Create another input for test data\n",
    "      # Note that eval_config was set locally\n",
    "      test_input = PoemInput(poems_test, eval_config, name=\"TestInput\")\n",
    "      # Create another model but reuse the variables in the training model\n",
    "      with tf.variable_scope(\"Model\", reuse=True):\n",
    "        mtest = MySeq2SeqModel(is_training=False, config=eval_config,\n",
    "                         input_=test_input)\n",
    "\n",
    "    with tf.name_scope(\"Gen\"):\n",
    "      generator_config = Hparam()\n",
    "      generator_config.batch_size = 1\n",
    "      generator_config.num_steps = 19\n",
    "      # Create generator model\n",
    "      with tf.variable_scope(\"Model\", reuse=True):\n",
    "        mgenerate = MyGeneratorModel(config=generator_config)\n",
    "\n",
    "    # Hardware settings\n",
    "    config_proto = tf.ConfigProto(allow_soft_placement=True)\n",
    "    # Create a MonitoredTrainingSession that controls the training process\n",
    "    # Also automatically logs and reports \n",
    "    # Note the `checkpoint_dir` setting\n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=\"logs\", \\\n",
    "                                           config=config_proto, \\\n",
    "                                           log_step_count_steps=-1) as session:\n",
    "      for i in range(config.num_epochs_to_train):\n",
    "\n",
    "        # Calculate learning rate decay\n",
    "        lr_decay = config.lr_decay ** max(i + 1 - config.warmup_epochs, 0.0)\n",
    "        # Set learning rate\n",
    "        m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "        # Print new learning rate\n",
    "        print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        # Train one epoch and report loss\n",
    "        train_perplexity = run_epoch(session, m, do_op=m.train_op, verbose=True)\n",
    "        print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "      \n",
    "      # End of training\n",
    "      # Evaluate test set performance\n",
    "      test_perplexity = run_epoch(session, mtest)\n",
    "      print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "      \n",
    "      # Set a seed word and generate new poem\n",
    "      seed_word = '天'\n",
    "      run_generator(session, mgenerate, seed_word=word_index[seed_word], config=generator_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_6KfSfUnbC5"
   },
   "source": [
    "## Start training\n",
    "We can actually start training by calling the controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3f2BoLtl9Yo"
   },
   "outputs": [],
   "source": [
    "main(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgU5M-MXVB9x"
   },
   "source": [
    "We can see the training process shown here. Observe that training loss keeps decreasing, which means that the model is actually learning. \n",
    "\n",
    "Also, due to the speedup of CudnnLSTM, the speed can be very fast (> 100,000 w/s). Using basic LSTM can only achieve ~6,000 w/s.\n",
    "\n",
    "If you are running this script locally, start `tensorboard` and point it to the `logs` directory will allow you to see the loss plot over time. We will not be able to show that easily in Colab environment.\n",
    "\n",
    "You can also continue training by calling the controller again. Try this later and see if the poems generated gets better over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rW284a7xioaw"
   },
   "source": [
    "## Clear previous output\n",
    "\n",
    "Tensorflow will automatically load previous models if you specify a path for the `session`. However, that will be a problem if you change some parts of the model. e.g., change embedding size, LSTM size, or number of layers.\n",
    "\n",
    "You will see something like \n",
    "```\n",
    "INFO:tensorflow:Restoring parameters from logs/model.ckpt-4465\n",
    "...\n",
    "InvalidArgumentError: Assign requires shapes of both tensors to match.\n",
    "```\n",
    "Always remember to clear output directory if you are experimenting with different model structures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfkNbkgyG81Z"
   },
   "outputs": [],
   "source": [
    "!rm -R logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWfT6DruhUBd"
   },
   "source": [
    "# Summary\n",
    "What we learned today:\n",
    "1. Preprocessing for language modeling data\n",
    "    * Create a dictionary that maps words to unique IDs\n",
    "    * Convert words to ID\n",
    "    * Reshape sequences to unified lengths\n",
    "    * Create a helper to produce data\n",
    "2. Building a model using tensorflow\n",
    "    * Hyperparameters\n",
    "    * Training operation\n",
    "    * Testing operation\n",
    "    * Control function\n",
    "3. Training and evaluation\n",
    "    * Observe loss\n",
    "    * Evaluate on test set\n",
    "\n",
    "You are now capable of building a deep learning model for a basic seq2seq task using **tensorflow**! \n",
    "\n",
    "However, tensorflow is extremely complicated (but powerful). There are numerous examples online for you to explore.\n",
    "\n",
    "## Extension\n",
    "\n",
    "Can you think of anything else that may also be learned using this model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDKza528g_pK"
   },
   "source": [
    "# Appendix: connect your Google Drive to Colab for uploading your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Z3N08zCdhJG"
   },
   "source": [
    "First, copy the file into Google Drive. Then run the following code to link your Drive to this notebook. Follow the link in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NI3J6rPSqRHP"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbRchpOzdgm2"
   },
   "source": [
    "Copy (`cp`) the file from `/gdrive` to this server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-ObQ860GsUw"
   },
   "outputs": [],
   "source": [
    "!cp /gdrive/My\\ Drive/Colab\\ Notebooks/poetry.txt ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EY_0LsbNK42a"
   },
   "outputs": [],
   "source": [
    "!cp /gdrive/My\\ Drive/Colab\\ Notebooks/Book*.txt ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Q4i6EI9KAy9"
   },
   "source": [
    "# Part III: Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAQQu-1jKNI0"
   },
   "source": [
    "## Task: Translation\n",
    "Hints on how to add attention in seq2seq model in order to perform translation. \n",
    "### CWMT corpus\n",
    "This is a Chinese-English translation dataset.\n",
    "\n",
    "Visit source website to download manually:\n",
    "http://nlp.nju.edu.cn/cwmt-wmt/\n",
    "\n",
    "Take a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BW-EFHSpdqQu"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell\n",
    "from tensorflow.nn import embedding_lookup, dropout\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-E6MEjLb-kV"
   },
   "outputs": [],
   "source": [
    "c_sents = [ss.strip() for ss in open('Book14_cn.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYVvCJ2OcO78"
   },
   "outputs": [],
   "source": [
    "c_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RD0_RQvZcXnV"
   },
   "outputs": [],
   "source": [
    "e_sents=[ss.strip() for ss in open('Book14_en.txt').readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1q3yPC_3cf6Y"
   },
   "outputs": [],
   "source": [
    "e_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ojo9ek5Zchvt"
   },
   "outputs": [],
   "source": [
    "c_tokenizer = Tokenizer(num_words=None, lower=False, char_level=True)\n",
    "# Create word to ID dictionary\n",
    "c_tokenizer.fit_on_texts(c_sents)\n",
    "# Get dictionary\n",
    "c_word_index = c_tokenizer.word_index\n",
    "# Fix word to ID\n",
    "c_word_index = {c:i+1 for c, i in c_word_index.items()}\n",
    "c_word_index[\"<PAD>\"] = 0\n",
    "c_word_index[\"<UNK>\"] = 1\n",
    "c_tokenizer.word_index = c_word_index\n",
    "c_reverse_word_index = dict([(v, k) for (k, v) in c_word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiWR9KS1c8rP"
   },
   "outputs": [],
   "source": [
    "# sort word index by ID\n",
    "for (w,i) in sorted(c_word_index.items(), key=lambda w: w[1]):\n",
    "# print some words to check if there are errors!\n",
    "  if i > 10 and i < len(c_word_index)-5: continue\n",
    "  print(\"{} {}\".format(w,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPWpPpDZdFBv"
   },
   "outputs": [],
   "source": [
    "e_vocab_size = 20000\n",
    "e_tokenizer = Tokenizer(num_words=e_vocab_size, lower=True, oov_token=\"<UNK>\")\n",
    "# Create word to ID dictionary\n",
    "e_tokenizer.fit_on_texts(e_sents)\n",
    "# Get dictionary\n",
    "e_word_index = e_tokenizer.word_index\n",
    "# Fix word to ID\n",
    "e_word_index = {e:i+1 for e, i in e_word_index.items() if i < e_vocab_size-1}\n",
    "e_word_index[\"<PAD>\"] = 0\n",
    "e_word_index[\"<UNK>\"] = 1\n",
    "e_tokenizer.word_index = e_word_index\n",
    "e_reverse_word_index = dict([(v, k) for (k, v) in e_word_index.items()])\n",
    "# sort word index by ID\n",
    "for (w,i) in sorted(e_word_index.items(), key=lambda w: w[1]):\n",
    "# print some words to check if there are errors!\n",
    "  if i > 10 and i < len(e_word_index)-5: continue\n",
    "  print(\"{} {}\".format(w,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_Fp-cEgRaEm"
   },
   "outputs": [],
   "source": [
    "c_sents = c_tokenizer.texts_to_sequences(c_sents)\n",
    "e_sents = e_tokenizer.texts_to_sequences(e_sents)\n",
    "c_sents = pad_sequences(c_sents,value=c_word_index[\"<PAD>\"], padding='post', truncating='post', maxlen=10)\n",
    "e_sents = pad_sequences(e_sents,value=e_word_index[\"<PAD>\"], padding='post', truncating='post', maxlen=10)\n",
    "train_data = (c_sents[:-5], e_sents[:-5])\n",
    "test_data = (c_sents[-5:], e_sents[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOSdLW6iSgUi"
   },
   "outputs": [],
   "source": [
    "train_data[0][0], train_data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvZG4nM807FB"
   },
   "source": [
    "### Change hyperparameters\n",
    "* Add separate vocabulary sized for English and Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77XNXnxlOeN-"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "class Hparam(object):\n",
    "  # ...\n",
    "  source_vocab_size = len(c_word_index)\n",
    "  target_vocab_size = len(e_word_index)\n",
    "  # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-oIyT8q2zKw"
   },
   "source": [
    "### Prepare input for translation\n",
    "\n",
    "* Modify `class PoemInput(object)` to create different source and target sentences. Most importantly, change the final part.\n",
    "* Use `pad_sequences` to pad both Chinese and English sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6vD46nKN3Wi"
   },
   "outputs": [],
   "source": [
    "class TranslationInput(object):\n",
    "  def __init__(self, data, config, name=None):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = config.num_steps\n",
    "    self.sources, self.targets = self.input_producer(\n",
    "        data, batch_size, name=name)\n",
    "\n",
    "  def input_producer(self, raw_data, batch_size, name=None):\n",
    "    source_data = tf.convert_to_tensor(raw_data[0], name=\"source_data\", dtype=tf.int32)\n",
    "    target_data = tf.convert_to_tensor(raw_data[1], name=\"target_data\", dtype=tf.int32)\n",
    "\n",
    "    num_batches = len(raw_data[0]) // self.batch_size\n",
    "    i = tf.train.range_input_producer(num_batches, shuffle=False).dequeue()\n",
    "    x = source_data[i*self.batch_size:(i+1)*self.batch_size, :]\n",
    "    y = target_data[i*self.batch_size:(i+1)*self.batch_size, :]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZse9WetxH96"
   },
   "source": [
    "## Build Translation Model\n",
    "This is also a seq2seq model, with some major differences:\n",
    "* Use one LSTM as the encoder\n",
    "* Add another as the decoder\n",
    "\n",
    "```python\n",
    "def _build_rnn_encoder\n",
    "    # RNN requires time-major\n",
    "    inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "    self._enccell = CudnnLSTM(\n",
    "        num_layers=config.num_layers,\n",
    "        num_units=config.hidden_size,\n",
    "        name=name)\n",
    "    outputs, state = self._enccell(inputs)\n",
    "    return outputs, state\n",
    "\n",
    "def _build_rnn_decoder\n",
    "    self._deccell = CudnnLSTM(\n",
    "            num_layers=config.num_layers,\n",
    "            num_units=config.hidden_size,\n",
    "            name=name)\n",
    "\n",
    "    outputs, state = self._deccell(inputs)\n",
    "    # Transpose from time-major to batch-major\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    return outputs, state\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pHL4dviMyCG8"
   },
   "source": [
    "### Modify training controller\n",
    "\n",
    "Use `TranslationInput` and `MyTranslationModel`.\n",
    "\n",
    "```python\n",
    "train_input = TranslationInput(train_data, config, name=\"TrainInput\")\n",
    "\n",
    "m = MyTranslationModel(is_training=True, config=config, input_=train_input)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SW46OsxK2OTD"
   },
   "source": [
    "### Snippet for adding attention mechanism in the model\n",
    "* Calculate attention score\n",
    "* Normalize score\n",
    "* Calculate context vector = *attention weighted sum*\n",
    "* Concatenate context vector with input\n",
    "* Use decoder to decode next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7aIxZfaTyDb"
   },
   "outputs": [],
   "source": [
    "# Require:\n",
    "# hidden: decoder hidden (memory)\n",
    "# enc_output: encoder output\n",
    "        \n",
    "# hidden shape == (batch_size, hidden size)\n",
    "# hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "# we are doing this to perform addition to calculate the score\n",
    "hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "# enc_output shape == (batch_size, max_length, hidden_size)\n",
    "# score shape == (batch_size, max_length, hidden_size)\n",
    "score = tf.nn.tanh(W1(enc_output) + W2(hidden_with_time_axis))\n",
    "\n",
    "# attention_weights shape == (batch_size, max_length, 1)\n",
    "# we get 1 at the last axis because we are applying score to V\n",
    "attention_weights = tf.nn.softmax(V(score), axis=1)\n",
    "\n",
    "# context_vector shape after sum == (batch_size, hidden_size)\n",
    "context_vector = attention_weights * enc_output\n",
    "context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "# x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "x = self.embedding(input_)\n",
    "\n",
    "# x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "# passing the concatenated vector to the decoder\n",
    "output, state = self.decoder(x)\n",
    "\n",
    "# output shape == (batch_size * 1, hidden_size)\n",
    "output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "# output shape == (batch_size * 1, vocab)\n",
    "x = SoftmaxLayer(output)\n",
    "\n",
    "# Output:\n",
    "# x: decoder output\n",
    "# state: decoder state\n",
    "# attention_weights: weights over encoder output at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ue2W5_FFbZze"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week11_NLP_codelab_Seq2seq",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
