{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# Part I: NLP Basics\n",
    "## Task: Sentiment analysis of movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "We begin our tutorial by introducing some basic steps involved in natural language processing (NLP) tasks. Our problem at hand is to classify movie reviews as *positive* or *negative* using the text of the review. \n",
    "It is an example of a binary classification, a fundamental and widely applicable kind of machine learning problem. \n",
    "\n",
    "A widely-used dataset for this task is the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb). \n",
    "It contains 50,000 movie reviews from IMDB, and preprocessed into\n",
    "25,000 reviews for training and 25,000 reviews for testing. \n",
    "Notice that both sets are balanced, which means that there are equal numbers of items in both classes.  \n",
    "\n",
    "You will first use a high-level framework for deep learning, [Keras](https://www.tensorflow.org/guide/keras), as the tool to build a model for this task. \n",
    "Keras helps us to leverage powerful backend toolkits such as Theano and [TensorFlow](https://www.tensorflow.org/).\n",
    "You may find that it is much easier to build a simple model for sentiment analysis using Keras than other complex toolkits.\n",
    "However, as you will see in the future courses, when the problem becomes more difficult, you need to dive into the finer mechanisms of deep learning toolkits.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, CuDNNLSTM\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling1D\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Download the IMDB dataset\n",
    "\n",
    "The IMDB dataset is conviently preprocessed by others and can be easily obtained using Keras. \n",
    "The reviews (sequences of words) have been converted to sequences of integers, where each integer represents a specific word in a dictionary. The dictionary is also pre-built. \n",
    "\n",
    "Keras contains the following helper function that downloads the IMDB dataset to your machine.\n",
    "\n",
    "```python\n",
    "def load_data(path='imdb.npz', num_words=None, skip_top=0, maxlen=None, seed=113,\n",
    "              start_char=1, oov_char=2, index_from=3, **kwargs):\n",
    "```\n",
    "\n",
    "We talked about the design challenge of setting a vocabulary size. For now, we will set it to 10,000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "       keras.datasets.imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odr-KlzO-lkL"
   },
   "source": [
    "The argument `num_words=vocab_size` keeps the top 10,000 most frequently occurring words in the training data. \n",
    "Other rare words are replaced by `oov_char` to keep the size of the model manageable. \n",
    "Recall that we have an embedding matrix that maps words to a vector.\n",
    "If there are 10,000 words each having a 100-dimension vector, that would take up 1,000,000, or 1 million parameters in our model. However, compared to sparse encoding which would have needed 100 million parameters, this is much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7FPWkGzYL_ro"
   },
   "source": [
    "Keras also comes with a pre-built dictionary of mapping words to its ID. However, it does not match the preprocessed word IDs. We need to add special words into this dictionary. `<PAD>` `<SOS>` `<UNK>` are added to match the settings `start_char=1, oov_char=2, index_from=3` in `load_data()`.\n",
    "\n",
    "It is common in NLP to add these special words in the dictionary. We want to add the *PADDING* symbol `<PAD>`, *Start-of-sentence* symbol `<SOS>`, and *Unknown* symbol `<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVBVl8g10y0d"
   },
   "outputs": [],
   "source": [
    "# A dictionary mapping words --> integer index\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# Shift word index by 3 because we want to add special words\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0  # padding\n",
    "word_index[\"<SOS>\"] = 1  # start of sequence\n",
    "word_index[\"<UNK>\"] = 2  # unknown (out of the top 10,000 most frequent words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXdoEKwi0y0h"
   },
   "source": [
    "For our convenience, we will create a helper function to convert integer IDs back to words. It is easier to find errors that way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-c6Anz8R0y0h"
   },
   "outputs": [],
   "source": [
    "# Build another dictionary of mapping integer --> words \n",
    "reverse_word_index = dict([(v, k) for (k, v) in word_index.items()])\n",
    "\n",
    "# Create a helper function to convert the integer to words\n",
    "# also limit max length\n",
    "def decode_review(text):\n",
    "  words = [reverse_word_index.get(i, \"<UNK>\") for i in text]\n",
    "  fixed_width_string = []\n",
    "  # limit max length = 10\n",
    "  for w_pos in range(len(words)):\n",
    "    fixed_width_string.append(words[w_pos])\n",
    "    if (w_pos+1) % 10 == 0:\n",
    "      fixed_width_string.append('\\n')\n",
    "  return ' '.join(fixed_width_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## Always check the content\n",
    "\n",
    "### Confirm the correctness of preprocessing, because things can go wrong in so many ways.\n",
    "\n",
    "When you deal with your own dataset, you have to write your own preprocessing procedure. Remember to check the correctness of the preprocessing!\n",
    "\n",
    "In our design, each input should contain a list of integers representing the words of the movie review, and output should be an integer of 0 or 1. We use 0 to represent a negative review and 1 positive. First we want to make sure the number of reviews and labels are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8qCnve_-lkO"
   },
   "outputs": [],
   "source": [
    "print(\"Training data: {} reviews, {} labels\".format(len(train_data), len(train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnKvHWW4-lkW"
   },
   "source": [
    "The words should be converted to integers, where each integer represents a specific word in a dictionary. Here's what the first review looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [],
   "source": [
    "print(\"Word IDs\")\n",
    "print(train_data[0])\n",
    "print(\"Label\")\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIE4l_72x7DP"
   },
   "source": [
    "Movie reviews may be of different lengths. We can see by examining a few of them. \n",
    "\n",
    "Since inputs must be the same length, we'll need to resolve this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-6Ii9Pfx6Nr"
   },
   "outputs": [],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3CNRvEZVppl"
   },
   "source": [
    "We can use the `decode_review` function to display the text for the first review, and also check for any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_OqxmH6-lkn"
   },
   "outputs": [],
   "source": [
    "print(decode_review(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2cv9iNJH5ATP"
   },
   "source": [
    "If we did **not** add special words in word_index, we will see that the reviews don't make any sense when using `decode_review`.  It is important to always check for errors like this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFP_XKVRp4_S"
   },
   "source": [
    "## Prepare the data for input\n",
    "\n",
    "Since the inputs must be of the same length, we will use the helper function `pad_sequences` in Keras to unify the lengths.\n",
    "\n",
    "Important options:\n",
    "* Max length = 100\n",
    "* The argument `padding='pre'` means that we are padding the beginning of a sentence.\n",
    "* If we set `maxlen=None`, Keras will automatically pad to __longest__ sequence in the dataset. \n",
    "* `truncating='pre'` indicates that the truncating will happen from the beginning of the review, so we are keeping the __last__ `maxlen=100` words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jQv-omsHurp"
   },
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        maxlen=100)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       truncating='pre',\n",
    "                                                       maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VO5MBpyQdipD"
   },
   "source": [
    "Let's look at the length of the examples now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USSSBnkE-lky"
   },
   "outputs": [],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJoxZGyfjT5V"
   },
   "source": [
    "And inspect the first review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TG8X9cqi-lk9"
   },
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTY6qJlZ0y06"
   },
   "source": [
    "Compare the review now vs. the original above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKibIdzq0y06"
   },
   "outputs": [],
   "source": [
    "print(decode_review(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ILVfiyM0y09"
   },
   "source": [
    "We can see that the review has been cut from the beginning, because we set `truncating='pre'` in `pad_sequences`.\n",
    "\n",
    "For shorter reviews, we can see that it has been padded from the beginning as in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSe732FT0y09"
   },
   "outputs": [],
   "source": [
    "print(decode_review(train_data[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Az6rbK10y0_"
   },
   "source": [
    "If you need to process your own dataset, you can use another convenient helper function in Keras\n",
    "```python\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<UNK>\")\n",
    "# SENTENCES = list of list of words\n",
    "tokenizer.fit_on_texts(SENTENCES)\n",
    "sequences = tokenizer.texts_to_sequences(SENTENCES)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "```\n",
    "\n",
    "Remember to call `pad_sequences` later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCWYwkug-llQ"
   },
   "source": [
    "## Create a validation set\n",
    "\n",
    "When training, we want to check the accuracy of the model on data it hasn't seen before. Create a *validation set* by setting apart some examples from the original training data. \n",
    "\n",
    "Why not use the testing set now? Our goal is to develop and tune our model using only the training data, then use the test data just once to evaluate our accuracy. \n",
    "\n",
    "To save time, we will use only a part of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NpcXY9--llS"
   },
   "outputs": [],
   "source": [
    "x_val = train_data[:1000]\n",
    "partial_x_train = train_data[1000:10000]\n",
    "\n",
    "y_val = train_labels[:1000]\n",
    "partial_y_train = train_labels[1000:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "Keras can help us build a model quickly. The neural network is created by adding layers. However, you need to decide :\n",
    "* How many layers?\n",
    "* How many hidden units to use for each layer?\n",
    "\n",
    "Let's build a simple model for the sentiment analysis problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpKOoWgu-llD"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# Embedding layer maps each of the 10000 words to 100-d embeddings\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "# Average the embeddings\n",
    "model.add(GlobalAveragePooling1D())\n",
    "# 1 Fully-connected layer\n",
    "model.add(Dense(16, activation=tf.nn.relu))\n",
    "# 2 Fully-connected layer\n",
    "model.add(Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PbKQ6mucuKL"
   },
   "source": [
    "### Layers\n",
    "In this example, the layers are linked sequentially, i.e., the output of the previous layer is sent to the next layer only.\n",
    "\n",
    "The first layer is an `Embedding` layer. This layer takes a sequence of word IDs (integer) and looks up an embedding matrix for a vector that represents that ID. **These vectors are learned as the model trains**. Note that, since this layer converts the 2D input of shape `(batch, sequence_len)` to `(batch, sequence_len, embedding_size)`.\n",
    "\n",
    "---\n",
    "\n",
    "The next layer, `GlobalAveragePooling1D` layer, calculates an average of the **second dimension**. So a batch of sequences of embeddings with shape `(batch, sequence_len, embedding_size)` will be averaged to a shape `(batch, embedding_size)`. \n",
    "\n",
    "---\n",
    "\n",
    "The last two layers are fully-connected (`Dense`) layer with 16  and 1 hidden unit(s).\n",
    "First fully-connected layer can be thought of as a feature reduction.\n",
    "The final layer is applying the `sigmoid` activation function, which has an output value between 0 and 1, to act as a probability or confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0XMwnDOp-llH"
   },
   "source": [
    "### Hidden units\n",
    "\n",
    "The above model has several intermediate or \"hidden\" layers, between the input and output. The number of \"units\" (or neurons) is the dimension of the representational space for the layer. In other words, the amount of freedom the network is allowed when learning an internal representation.\n",
    "\n",
    "If a model has more hidden units (a higher-dimensional representation space), and/or more layers, then the network can learn more complex representations. However, it makes the network more computationally expensive and may lead to learning unwanted patterns—patterns that improve performance on training data but not on the test data. This is called *overfitting*, and we'll explore it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4EqVWg4-llM"
   },
   "source": [
    "### Loss function and optimizer\n",
    "\n",
    "A model needs a loss function and an optimizer for training. Since this is a **binary** classification problem and the model outputs of a probability (a single-unit layer with a `sigmoid` activation function), we'll use the `binary_crossentropy` loss function. \n",
    "\n",
    "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "If you are handling a classification problem with more that two classes, you will need to set the final layer to have the same number of units as your classes. The model then outputs probabilities of each class (using a `softmax` activation function). Then, you will need to use the `categorical_crossentropy` loss function. \n",
    "\n",
    "When you are dealing with regression problems (say, to predict the price of a house), you  will need to use other loss functions such as `mean_squared_error`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The design of Keras requires us to configure the loss and optimizer of the model together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35jv_fzP-llU"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Train the model for *n* epochs in mini-batches of samples. Recall that this is *n* iterations over all samples in the training data. While training, monitor the model's loss and accuracy on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXSGrjWZ-llW"
   },
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=8,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## Evaluate the model\n",
    "\n",
    "And let's see how the model performs on the test set. Two values will be returned when calling `.evaluate` function, **loss** (we defined it as binary cross entropy) and **accuracy**.  Keras will report whatever we used in the `model.compile` function as the evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOMKywn4zReN"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1iEXVTR0Z2t"
   },
   "source": [
    "This fairly naive approach achieves an accuracy of 83%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KggXVeL-llZ"
   },
   "source": [
    "## Create a plot of accuracy and loss over time\n",
    "\n",
    "`model.fit()` returns a `History` object that contains a dictionary with everything that happened during training. There are four entries: one for each monitored metric during training and validation. We can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy. We will write a helper function to plot loss and accuracy of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcvSXvhp-llb"
   },
   "outputs": [],
   "source": [
    "def plot_hist(history):\n",
    "    history_dict = history.history\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    # plot for loss\n",
    "    plt.clf()   # clear figure\n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    # r is for \"red solid line\"\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plot for accuracy\n",
    "    plt.clf()   # clear figure\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFEmZ5zq-llk"
   },
   "source": [
    "In this plot, the dots represent the training loss and accuracy, and the solid lines are the validation loss and accuracy.\n",
    "\n",
    "Notice the training loss *decreases* with each epoch and the training accuracy *increases* with each epoch. This is expected because we designed the model to optimize for this goal.\n",
    "\n",
    "However, the loss and accuracy for validation data is usually different from the training data. They usually peak after some epochs. \n",
    "\n",
    "Recall that we have talked about **overfitting**: the model performs much better on the training data than it does on new data. \n",
    "After this point, the model over-optimizes and learns representations *specific* to the training data that do not *generalize* to test data.\n",
    "\n",
    "We could prevent overfitting by simply stopping the training after some epochs by observing these plots. Also,  we could apply a simple method that uses `Dropout`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ziQOD8R_VltN"
   },
   "source": [
    "## Deal with overfitting\n",
    "A very straightforward method is to insert `Dropout` layers in between our previous layers.  \n",
    "\n",
    "**Important**: `Dropout(rate)` rate is a float between 0 and 1 that indicates the fraction of the input units to **drop**. \n",
    "\n",
    "However, in `tensorflow`, the dropout layers take an argument of `keep_prob` which indicates the fraction to **keep**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ZkJeRBaUwAf"
   },
   "outputs": [],
   "source": [
    "# Clear previous model\n",
    "model = None\n",
    "K.clear_session()\n",
    "model = keras.Sequential()\n",
    "# Embedding layer maps each of the 10000 words to 100-d embeddings\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "# Average the embeddings\n",
    "model.add(GlobalAveragePooling1D())\n",
    "# 1 Fully-connected layer\n",
    "model.add(Dense(16, activation=tf.nn.relu))\n",
    "\n",
    "# Dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 2 Fully-connected layer\n",
    "model.add(Dense(1, activation=tf.nn.sigmoid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWTze2ODU9Ae"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=12,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val, y_val))\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tclUzlb_VNl3"
   },
   "source": [
    "Compare the two plots from two models, it seems that `Dropout` is effective at reducing overfitting!\n",
    "\n",
    "\n",
    "Also remember that when applying dropout, we usually need to train for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGtlC54T_ydW"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_UhWQ00ASV21"
   },
   "source": [
    "## Build a recurrent model\n",
    "\n",
    "You can easily change to a recurrent model in Keras. Simply replace `Dense` with `LSTM` and that is it! \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Specifically, some parameters can be set for the LSTM cell.\n",
    "\n",
    "```python\n",
    "LSTM(hidden_units, dropout=0.0, recurrent_dropout=0.0)\n",
    "```\n",
    "\n",
    "The first `dropout` refers to the dropping of input features, and `recurrent_dropout` refers to the dropping of the previous output. Recall that in the slides we showed some connection between the previous output and the current input?\n",
    "\n",
    "![dropout_difference](https://drive.google.com/uc?export=view&id=1kiiV6BvPalvGnA6zg3LioDHOMs44TBrr)\n",
    "\n",
    "Let's build a recurrent model for the sentiment analysis problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzruqyZT0y1T"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "K.clear_session()\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "\n",
    "# Add a recurrent layer\n",
    "# model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.5))\n",
    "## or CuDNNLSTM\n",
    "model.add(CuDNNLSTM(32))\n",
    "##\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1wAcMvnirFd"
   },
   "source": [
    "Again, we build the model and train for some epochs. Observe the change of loss and accuracy compared with the previous model.\n",
    "\n",
    "**Note**: due to speed issues we will train for 4 epochs when using recurrent models. You can try changing that later! Also, you can use `CuDNNLSTM` which is the latest LSTM version in tensorflow that is optimized for GPU. However, it does **not** support dropout itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8wdkMR5X0y1W"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train, \\\n",
    "                    partial_y_train, \\\n",
    "                    epochs=8, \\\n",
    "                    batch_size=100, \\\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOzGh0u_ixer"
   },
   "source": [
    "We can see a very large decrease in speed. (**55us/step** vs. **3ms/step**, which is about **55** times slower)\n",
    "\n",
    "And evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XU531arM0y1X"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1RZdQrPi2Jn"
   },
   "source": [
    "### Improvements to a recurrent model\n",
    "First, we will try to make it *deeper* by adding more LSTMs. \n",
    "\n",
    "**We must change the previous layers!!**\n",
    "\n",
    "By default, a recurrent cell only returns the **last** output. But for multiple layers of recurrent cells, we need the **entire** output. \n",
    "\n",
    "So, we have to set `return_sequences=True` for all recurrent layers except the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tK3IJLB70y1Z"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "K.clear_session()\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_size, 32))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=False))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xp7bJHVr0y1b"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val, y_val))\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xNM9Kwtwjz_"
   },
   "source": [
    "Next, we will try using *bi-directional* RNN. It is also very easy, just wrap your LSTM cell in `Bidirectional()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myGtsU2V0y1d"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "K.clear_session()\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_size, 32))\n",
    "\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12qyTBs50y1h"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val, y_val))\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxMNmfRLaYdK"
   },
   "source": [
    "Can you think of our next move? \n",
    "\n",
    "We can combine deep LSTM with bidirectional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGPm6FPMaMA2"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "K.clear_session()\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_size, 32))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FdeVWh05aNwb"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val, y_val))\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZT9RLs0McpeB"
   },
   "source": [
    "We can see that it is even slower than our previous model (about 3 times slower), but the loss and accuracy are better than previous models.\n",
    "\n",
    "Remember that there is no guarantee that deeper or more complex models perform better than simple models, especially when you don't have enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "osRVUOrpw0h6"
   },
   "source": [
    "## Pretrained embeddings\n",
    "Finally, we will try using **pretrained embeddings**.\n",
    "Pretrained embeddings are not limited to recurrent models. They can be very useful in almost every NLP applications. \n",
    "\n",
    "The reason? \n",
    "\n",
    "Instead of randomly initialize the word embeddings, we can train them using a huge amout of (unlabeled) data in an unsupervised fashion.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "We will use the [GloVe embeddings](http://nlp.stanford.edu/projects/glove/) to initialize our embedding weights. \n",
    "The file format is \n",
    "```\n",
    "word1  0.xx -0.xx ... \n",
    "word2  0.yy 0.yy ...\n",
    "```\n",
    "Here, we read the entire file and see which of those words are in our dictionary. If we find one, we will set its embedding accordingly. Other words that are not found in the pretrained embeddings can be initialized as all zeros or very small numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "axRK5B4M0y1i"
   },
   "outputs": [],
   "source": [
    "## Download pretrained GloVe embeddings from kaggle\n",
    "## !kaggle datasets download -d terenceliu4444/glove6b100dtxt\n",
    "## !unzip glove6b100dtxt.zip\n",
    "\n",
    "## Randomize pretrained embeddings matrix with very small numbers\n",
    "pretrained_embedding_matrix = (np.random.rand(vocab_size, 100) - 0.5) / 1e4\n",
    "## Initialize embeddings matrix to all zeros\n",
    "# pretrained_embedding_matrix = np.zeros((vocab_size, 100))\n",
    "## Load pretrained embeddings\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in word_index:\n",
    "            embs = np.asarray(values[1:], dtype='float32')\n",
    "            if word_index[word] >= vocab_size: continue\n",
    "            pretrained_embedding_matrix[word_index[word]] = embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBZLuGia0y1l"
   },
   "source": [
    "`pretrained_embedding_matrix` now contains pre-trained embeddings and can be used to initialize the embedding layer.\n",
    "\n",
    "An option is to set the embeddings to `trainable=False` which stops them from begin updated during training. \n",
    "However, this is not always useful as the pretrained embeddings may come from a **different dataset**. \n",
    "\n",
    "If you have a large amount of unlabeled data and a smaller labeled data, both from the same source, you can consider setting the `trainable=False` flag. This has an additional benefit of **reducing the number of parameters**, which in turn reduces the amount of training data that you need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rn6zAX70Oi9q"
   },
   "source": [
    "We can test the effect of pretrained embeddings on our very simple model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrjAvrniOpnV"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "K.clear_session()\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Embedding layer with pretrained embeddings\n",
    "model.add(Embedding(vocab_size, 100, weights=[pretrained_embedding_matrix]))\n",
    "\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation=tf.nn.relu))\n",
    "model.add(Dense(1, activation=tf.nn.sigmoid))\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=8,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val, y_val))\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGsbe0AHQHbZ"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCsQRCZTQoLd"
   },
   "source": [
    "We can see that the model improved slightly from 83% to 84%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZmuk3xXO7IE"
   },
   "source": [
    "Next we will try it on the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fIy13Nl0y1m"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "K.clear_session()\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[pretrained_embedding_matrix]))\n",
    "model.add(CuDNNLSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val, y_val))\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYTOfSqk0y1o"
   },
   "source": [
    "Finally, we test its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4Pd3IfW0y1p"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNKs6uwCSJEX"
   },
   "source": [
    "# Summary\n",
    "What we learned today:\n",
    "1. Basic preprocessing of NLP data\n",
    "    * Tokenize words\n",
    "    * Create a dictionary that maps words to unique IDs\n",
    "    * Convert words to ID\n",
    "    * Pad/truncate sequences to unified lengths\n",
    "2. Building a model using Keras\n",
    "    * Design model structure\n",
    "    * Add layers\n",
    "    * Define loss\n",
    "    * Define optimizer\n",
    "3. Training and evaluation\n",
    "    * Create a plot to clearly observe training progress\n",
    "    * Evaluate on test set\n",
    "4. Improvements to the model\n",
    "    * Dropout\n",
    "    * Recurrent\n",
    "    * Pretrained embeddings\n",
    "    * Mixture of the above\n",
    "\n",
    "You are now capable of building a deep learning model for NLP classisification tasks! Please modify this codelab later to use the entire training data and construct different model structures.  However, do not build overly large models because 1) it is not always effective ans 2) it may not run on this cloud platform.\n",
    "\n",
    "## Extension\n",
    "Multi-class text classification using [reuters](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/reuters) dataset in tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BolHeqAi30nW"
   },
   "source": [
    "# Appendix: How to setup Kaggle on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUkaPYeQ1K3v"
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fm9SB_4Y3jrm"
   },
   "source": [
    "Download Kaggle API key and upload it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUKxfqUR3WdQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "k_config = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_AdEdAsV3fKC"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suJYTkmH1Qco"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d terenceliu4444/glove6b100dtxt\n",
    "!unzip glove6b100dtxt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xNeXePwYsz_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week10_NLP_codelab.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
